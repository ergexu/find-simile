{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c281bf-c797-43f0-b69a-4a21f66e5d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xumingkai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Make sure to download 'punkt' tokenizer models\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def find_sentences_and_chapters(text):\n",
    "    chapters = re.split(r'CHAPTER\\s+[IVXLCDM]+', text)\n",
    "    sentences_with_chapters = []\n",
    "    chapter_number = 0\n",
    "    \n",
    "    for chapter in chapters[1:]: \n",
    "        chapter = re.sub(r'\\s+', ' ', chapter).strip()\n",
    "        sentences = sent_tokenize(chapter)  \n",
    "        chapter_number += 1\n",
    "        sentences_with_chapters += [(preprocess_text(sentence), chapter_number) for sentence in sentences]\n",
    "    return sentences_with_chapters\n",
    "\n",
    "def save_sentences_to_csv(sentences, output_file):\n",
    "    df = pd.DataFrame(sentences, columns=['Sentence', 'Chapter'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def process_book_text_to_csv(input_file_path, output_csv_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        start_marker = \"*** start of the project gutenberg ebook\"\n",
    "        end_marker = \"*** end of the project gutenberg ebook\"\n",
    "        start_index = content.lower().find(start_marker) + len(start_marker)\n",
    "        end_index = content.lower().find(end_marker)\n",
    "        \n",
    "        main_content_start = content.find(\"***\", start_index) + 3\n",
    "        main_content = content[main_content_start:end_index].strip()\n",
    "        \n",
    "        sentences_with_chapters = find_sentences_and_chapters(main_content)\n",
    "    \n",
    "    save_sentences_to_csv(sentences_with_chapters, output_csv_file)\n",
    "\n",
    "    print(\"Process complete. Output saved to:\", output_csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cab93d-53b3-478d-93c2-ff9bfa1206fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_potential_simile(token):\n",
    "    \n",
    "    if token.dep_ in [\"prep\", \"mark\"] and token.head.pos_ in [\"VERB\", \"ADJ\"]:\n",
    "        for child in token.children:\n",
    "            if child.pos_ in [\"NOUN\", \"ADJ\", \"PROPN\"]:\n",
    "                return True\n",
    "    return False\n",
    "def extract_simile_components(sentence, chapter):\n",
    "    \n",
    "    if not isinstance(sentence, str):\n",
    "        return [] \n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    similes = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in [\"like\", \"as\"] and is_potential_simile(token):\n",
    "            tenor = token.head.text\n",
    "            vehicle = \" \".join([child.text for child in token.children if child.dep_ in [\"pobj\"]])\n",
    "            if vehicle:  \n",
    "                similes.append({\"Chapter\": chapter, \"Tenor\": tenor, \"Vehicle\": vehicle, \"Sentence\": sentence})\n",
    "    return similes\n",
    "\n",
    "def process_csv_and_find_similes(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv).fillna('')\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['Sentence']\n",
    "        chapter = row['Chapter']  \n",
    "        similes = extract_simile_components(sentence, chapter) \n",
    "        results.extend(similes)\n",
    "\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Simile analysis results saved to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No similes found in the input data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57d8c2-f57d-48f2-98ab-009597e80421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file_path = 'Piccadilly Jim.txt'\n",
    "output_csv_path = 'Piccadilly Jim_sentences_with_chapters.csv'\n",
    "process_book_text_to_csv(input_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01373c29-4d54-4155-b8fb-47264c9d0581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_csv_path = 'Piccadilly Jim_sentences_with_chapters.csv'  \n",
    "output_csv_path = 'Piccadilly Jim_simile.csv'  \n",
    "\n",
    "process_csv_and_find_similes(input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20583d26-a6f5-4d3c-82a4-d23bc90293e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
